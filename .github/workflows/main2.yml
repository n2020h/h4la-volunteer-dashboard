name: Scrape and Commit Data

# Controls when the workflow will run
on:
  push:  # Triggers the workflow on a push to any branch
  workflow_dispatch:  # Allows the workflow to be manually triggered
  schedule:
    - cron: '*/5 * * * *'  # Schedules the workflow to run every 5 minutes

jobs:
  scrape_and_commit:
    # Specifies the type of virtual environment the job will run on
    runs-on: ubuntu-latest

    steps:
    # Step 1: Checkout the repository
    - name: Checkout repository
      uses: actions/checkout@v3
      # Checks out your repository under $GITHUB_WORKSPACE, so your job can access it

    # Step 2: Fetch the latest data
    - name: Fetch latest data
      run: curl "https://docs.google.com/spreadsheets/d/1Okj4dVMIf780yfdFO0qxs-M1fmt72l3smYzF4aB74b8/edit?usp=sharing" -o volunteer_table.csv
      # Uses curl to download data from the specified URL and saves it as usgs_most_recent.csv

    # Step 3: Commit and push if there are changes
    - name: Commit and push
      run: |
        git config user.name "Automated"
        git config user.email "actions@users.noreply.github.com"
        # Sets Git configuration for the commit

        git add volunteer_table.csv
        # Stages the downloaded file for commit

        timestamp=$(date -u)
        # Saves the current UTC time in a variable

        git commit -m "Latest data: ${timestamp}" || exit 0
        # Commits the changes with a timestamp. '|| exit 0' ensures the workflow doesn't fail if there's nothing to commit

        git push origin HEAD:${{ github.ref }}
        # Pushes the commit to the same branch that triggered the workflow
